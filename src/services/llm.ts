import OpenAI from 'openai';
import fs from 'fs/promises';
import path from 'path';

/**
 * LLM ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ í˜ì´ë¡œë“œ ì¸í„°í˜ì´ìŠ¤
 */
export interface ReportPayload {
  lookback_days: number;
  portfolio: {
    cash_usd: number;
    holdings: Array<{
      symbol: string;
      shares: number;
      avg_cost: number;
    }>;
  };
  market: {
    date: string;
    sector_code: string;
    sector_title: string;
  };
  indicators: Record<string, {
    close: number;
    ema20: number;
    ema50: number;
    rsi14: number;
  }>;
  news: Array<{
    published_at: string;
    source: string;
    title: string;
    url: string;
    summary: string;
    sentiment: number;
    relevance: number;
  }>;
  scores: Record<string, number>;
}

/**
 * OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
 */
let openai: OpenAI | null = null;

function getOpenAIClient(): OpenAI {
  if (!openai) {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }
    openai = new OpenAI({ apiKey });
  }
  return openai;
}

/**
 * OpenAI GPTë¥¼ ì‚¬ìš©í•œ ë³´ê³ ì„œ ìƒì„±
 */
export async function generateReportWithOpenAI(payload: ReportPayload): Promise<string> {
  try {
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    const promptPath = path.join(process.cwd(), 'prompt.md');
    const systemPrompt = await fs.readFile(promptPath, 'utf8');

    // LLM ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’ì„ gpt-5ë¡œ ë³€ê²½)
    const model = process.env.LLM_MODEL || 'gpt-5';

    console.log(`ğŸ¤– ${model}ì„ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œ ìƒì„± ì‹œì‘`);

    const messages = [
      {
        role: 'system' as const,
        content: systemPrompt
      },
      {
        role: 'user' as const,
        content: JSON.stringify(payload, null, 2)
      }
    ];

    const client = getOpenAIClient();
    
    // GPT-5 ëª¨ë¸ìš© íŒŒë¼ë¯¸í„° ì„¤ì •
    const isGpt5 = model.startsWith('gpt-5');
    const requestParams: any = {
      model: model,
      messages: messages,
    };
    
    // GPT-5 vs ê¸°ì¡´ ëª¨ë¸ íŒŒë¼ë¯¸í„° êµ¬ë¶„
    if (isGpt5) {
      // GPT-5 ì „ìš© íŒŒë¼ë¯¸í„° - reasoning_effortë¥¼ lowë¡œ ë‚®ì¶°ì„œ ì‹¤ì œ ì¶œë ¥ í† í° í™•ë³´
      requestParams.max_completion_tokens = 12000; // reasoning + ì‹¤ì œ ì¶œë ¥ì„ ìœ„í•œ ì¶©ë¶„í•œ í† í°
      requestParams.reasoning_effort = "low"; // lowë¡œ ì„¤ì •í•˜ì—¬ ì¶œë ¥ í† í° í™•ë³´
      // temperatureëŠ” ì„¤ì •í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’ 1 ì‚¬ìš©)
    } else {
      // ê¸°ì¡´ GPT ëª¨ë¸ íŒŒë¼ë¯¸í„°
      requestParams.temperature = 0.2;
      requestParams.max_tokens = 4000;
    }
    
    const response = await client.chat.completions.create(requestParams);

    console.log('ğŸ“Š OpenAI ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹…:', {
      choices_length: response.choices?.length || 0,
      first_choice: response.choices?.[0] ? {
        message_exists: !!response.choices[0].message,
        content_length: response.choices[0].message?.content?.length || 0,
        finish_reason: response.choices[0].finish_reason
      } : null
    });

    const report = response.choices?.[0]?.message?.content;
    
    if (!report || report.trim().length === 0) {
      console.error('âŒ OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŒ:', JSON.stringify(response, null, 2));
      throw new Error('LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤');
    }

    console.log('âœ… LLM ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ');
    return report;

  } catch (error) {
    console.error('âŒ LLM ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨:', error);
    
    // ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë³´ê³ ì„œ ë°˜í™˜
    return generateFallbackReport(payload);
  }
}

/**
 * LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ë³´ê³ ì„œ ìƒì„±
 */
function generateFallbackReport(payload: ReportPayload): string {
  const { market, portfolio, indicators, news } = payload;
  
  return `# ğŸ“Š ë°ì¼ë¦¬ ë¦¬í¬íŠ¸ â€“ ${market.date} (ì„¹í„°: ${market.sector_title})

## ìš”ì•½
- í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ê³„ì‚° ì¤‘...
- í˜„ê¸ˆ ë³´ìœ : $${portfolio.cash_usd.toFixed(2)}
- ë³´ìœ  ì¢…ëª© ìˆ˜: ${portfolio.holdings.length}ê°œ
- ì„¹í„° ëª¨ë©˜í…€: ë¶„ì„ ì¤‘...

## ì£¼ë¬¸ ì œì•ˆ
í˜„ì¬ LLM ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ì–´ ìë™ ì œì•ˆì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ìˆ˜ë™ìœ¼ë¡œ ì‹œì¥ ìƒí™©ì„ ê²€í† í•´ ì£¼ì„¸ìš”.

## ë³´ìœ  ì¢…ëª© ìƒíƒœ
${portfolio.holdings.map(holding => 
  `- ${holding.symbol}: ${holding.shares}ì£¼ (í‰ë‹¨ê°€: $${holding.avg_cost.toFixed(2)})`
).join('\n')}

## ì„¹í„° ë‰´ìŠ¤ Top ${Math.min(news.length, 5)}
${news.slice(0, 5).map(item => 
  `- ${item.title} (${item.source})`
).join('\n')}

## ë©”ì„œë“œ
- ì§€í‘œ: EMA(20/50), RSI(14)
- ê¸°ê°„: ìµœê·¼ ${payload.lookback_days}ì¼
- ìƒíƒœ: LLM ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨

> *ë³¸ ë¦¬í¬íŠ¸ëŠ” íˆ¬ììë¬¸ì´ ì•„ë‹ˆë©°, ëª¨ë“  ê²°ì •ê³¼ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.*`;
}

/**
 * GPT-5-nanoë¥¼ ì‚¬ìš©í•œ ë¦¬í¬íŠ¸ ìš”ì•½ ìƒì„±
 */
export async function generateReportSummary(reportContent: string): Promise<string> {
  try {
    const summaryPrompt = `ë‹¤ìŒ íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ íˆ¬ì í¬ì¸íŠ¸ì™€ ì¶”ì²œ ì¢…ëª©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:

${reportContent.substring(0, 2000)}...`; // ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì¼ë¶€ë§Œ ì‚¬ìš©

    console.log('ğŸ¤– GPT-5-nanoë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìš”ì•½ ìƒì„± ì‹œì‘');

    const messages = [
      {
        role: 'system' as const,
        content: 'íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.'
      },
      {
        role: 'user' as const,
        content: summaryPrompt
      }
    ];

    const client = getOpenAIClient();
    
    const response = await client.chat.completions.create({
      model: 'gpt-5-nano', // GPT-5-nano ì‚¬ìš© (ìš”ì•½ìš©)
      messages: messages,
      max_completion_tokens: 500, // reasoning + ì¶œë ¥ì„ ìœ„í•œ ì¶©ë¶„í•œ í† í°
      reasoning_effort: "low" // ë‚®ì€ ì¶”ë¡  ë ˆë²¨ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
      // temperatureëŠ” ì„¤ì •í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’ 1 ì‚¬ìš©)
    });

    const summary = response.choices[0].message?.content;
    
    if (!summary) {
      throw new Error('ìš”ì•½ ìƒì„± ì‹¤íŒ¨');
    }

    console.log('âœ… GPT-5-nano ìš”ì•½ ìƒì„± ì™„ë£Œ');
    return summary;

  } catch (error) {
    console.error('âŒ GPT-5-nano ìš”ì•½ ìƒì„± ì‹¤íŒ¨:', error);
    
    // ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½ ë°˜í™˜
    return 'ë¦¬í¬íŠ¸ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }
}

/**
 * ë‹¤ë¥¸ LLM ì œê³µì ì§€ì› (í–¥í›„ í™•ì¥ìš©)
 */
export async function generateReport(payload: ReportPayload): Promise<string> {
  const provider = process.env.LLM_PROVIDER || 'OPENAI';

  switch (provider.toUpperCase()) {
    case 'OPENAI':
      return await generateReportWithOpenAI(payload);
    
    // TODO: ë‹¤ë¥¸ LLM ì œê³µì ì¶”ê°€
    // case 'ANTHROPIC':
    //   return await generateReportWithAnthropic(payload);
    
    default:
      console.warn(`âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” LLM ì œê³µì: ${provider}, OpenAI ì‚¬ìš©`);
      return await generateReportWithOpenAI(payload);
  }
}